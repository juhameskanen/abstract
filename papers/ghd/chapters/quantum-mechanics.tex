\chapter{Why Quantum Mechanics?}

\section{The Strange Quantum World}

Quantum Mechanics is extraordinarily successful at describing microscopic phenomena. 
Its predictions have been confirmed to astonishing precision. Yet conceptually, 
it resists classical intuition.

At small scales, matter does not behave like tiny billiard balls. 
Instead, it behaves according to a complex-valued object called the \emph{wavefunction}.

\subsection{The Wavefunction}

Every isolated quantum system is described by a state vector 
\[
|\psi\rangle \in \mathcal{H},
\]
where $\mathcal{H}$ is a Hilbert space. 

The wavefunction evolves deterministically according to the Schrödinger equation:

\[
i\hbar \frac{\partial}{\partial t} |\psi(t)\rangle = \hat{H} |\psi(t)\rangle.
\]

This evolution is linear and unitary. No randomness appears at this level.

However, when we perform a measurement, we do not observe a superposition. 
We observe a definite outcome.

The double-slit experiment illustrates this vividly. 
Photons sent one by one through two slits produce an interference pattern. 
No classical particle passing through one slit at a time could generate such a pattern. 
What propagates through both slits is the \emph{wavefunction}. 
The particle becomes localized only upon detection.

Thus, there is a dual structure:

\begin{itemize}
\item Continuous, distributed wavefunction evolution.
\item Discrete, localized detection events.
\end{itemize}

This is particle–wave duality.

\subsection{Superposition as Informational Encoding}

Consider a quantum two-state system:

\[
|\psi\rangle = \alpha |H\rangle + \beta |T\rangle,
\quad
|\alpha|^2 + |\beta|^2 = 1.
\]

The system is not “half head and half tail” in a classical sense. 
Instead, the wavefunction encodes \emph{both possibilities simultaneously} 
within a single mathematical object.

Superposition allows interference. 
Amplitudes combine before probabilities are extracted:

\[
P = |\alpha + \beta|^2.
\]

This structure is crucial. The wavefunction does not store outcomes separately. 
It stores them in compressed, phase-sensitive form.

If one were to list all classical alternatives explicitly, 
the information content would scale combinatorially. 
Instead, the Hilbert space vector stores all alternatives 
in a linear superposition.

This is compression.

\subsection{Missing Identity}

Quantum particles lack classical individuality. 
Electrons are indistinguishable not merely in practice, but in principle. 
Their identity resides in the quantum state they occupy, not in a label.

In Hilbert space, exchanging two identical particles corresponds 
to an operator acting on the state vector.

For fermions:

\begin{equation}
\psi(x_1, x_2) = -\psi(x_2, x_1).
\end{equation}

If $x_1 = x_2$, then:

\begin{equation}
\psi(x, x) = 0.
\end{equation}

The Pauli exclusion principle follows directly. 
No two identical fermions can occupy the same quantum state.

For bosons:

\begin{equation}
\psi(x_1, x_2) = +\psi(x_2, x_1).
\end{equation}

Multiple occupation is allowed, enabling coherent states and 
Bose–Einstein condensation.

Identity is therefore encoded algebraically, not geometrically. 
The symmetry properties of Hilbert space replace classical individuality.

\subsection{Uncertainty and Localization}

The Born rule states:

\[
P(x,t) = |\psi(x,t)|^2.
\]

Probability is extracted from amplitude magnitude.

Consider a plane wave:

\[
\psi(x) = e^{ikx}.
\]

Momentum is precise: $p = \hbar k$.  
But position is completely delocalized.

To localize a particle, we must superpose many momenta:

\[
\psi(x) = \int a(k)e^{ikx} dk.
\]

The sharper the position localization, the broader the momentum distribution.

This is the content of the uncertainty relation:

\[
\Delta x \Delta p \ge \frac{\hbar}{2}.
\]

Localization requires informational expansion in momentum space.

In other words, precise position requires many Fourier components. 
The wavefunction distributes information between conjugate variables.

This trade-off is not experimental limitation. 
It is structural compression.

\subsection{Entanglement}

Consider two particles in the joint state:

\[
|\Psi\rangle = \frac{1}{\sqrt{2}} 
\left(
| \uparrow \downarrow \rangle 
-
| \downarrow \uparrow \rangle
\right).
\]

This state cannot be factorized:

\[
|\Psi\rangle \neq |\psi_1\rangle \otimes |\psi_2\rangle.
\]

The system is described by a single vector in a tensor-product space. 
The subsystems do not possess independent states.

Entanglement does not transmit signals faster than light. 
Instead, it reflects non-factorizable information encoding.

A useful analogy from computer science is shared memory. 
Two readouts accessing the same underlying data will exhibit correlated behavior, 
not because information travels between them, 
but because they were never independent.

Entanglement is not a force.  
It is a structural property of Hilbert space.

\section{Quantum Mechanics as Compression}

We may now ask:

Why does nature use Hilbert space at all?

A classical description would require listing 
every possible configuration explicitly. 
For $N$ binary variables, this requires $2^N$ classical states.

Quantum mechanics encodes all these alternatives into a single vector:

\[
|\psi\rangle = \sum_i c_i |i\rangle.
\]

The amplitudes $c_i$ store exponentially many classical possibilities 
within a linear structure.

Interference allows redundant branches to cancel.

Unitary evolution preserves total information, 
while compressing alternative histories into phase relationships.

Measurement extracts a single classical branch.

Thus:

\begin{itemize}
\item Hilbert space stores possibilities.
\item Interference compresses alternatives.
\item Measurement decompresses into classical outcomes.
\end{itemize}

Particle-like events are what we call reality.  
The wavefunction is the compression format.

\section{Conclusions}

What is the system we have described?

Consider a digital rendering engine. While a scene may contain continuous color values, the display device has only finite precision. To approximate intermediate values, the renderer employs \textbf{probabilistic dithering}.
Over many pixels, discrete outputs approximate a continuous tone.

Similarly, the wavefunction is continuous and complex-valued, yet measurement produces discrete events. The \textbf{Born probabilities} act as a weighted sampling of an underlying amplitude.
This suggests that classical outcomes are coarse-grained projections of a richer informational structure.

In this framework, particles function like specialized data structures:
\begin{itemize}
    \item \textbf{Fermions} resemble exclusive memory cells: one state, one occupant.
    \item \textbf{Bosons} resemble accumulative registers: amplitudes add coherently.
\end{itemize}

A concept that is used in every graphics hardware these days.

The quantum formalism organizes information so that exponentially many classical alternatives can be encoded compactly. The wavefunction maps all possible classical configurations into a single linear object in Hilbert space.

In this light:
\begin{itemize}
    \item \textbf{Superposition} compresses alternatives.
    \item \textbf{Interference} removes redundancy.
    \item \textbf{Measurement} extracts discrete events.
\end{itemize}


This perspective finds a analogy in digital signal processing, specifically within the \textbf{JPEG} and \textbf{MPEG} codecs. These standards rely on the Discrete Cosine Transform (DCT) to
decompose an image or video frame into a sum of periodic functions. By prioritizing low-frequency coefficients and discarding less perceptible data,
the codec compresses vast amounts of visual information into a manageable stream. 

Consider an observer existing entirely within the pixel grid of a highly compressed MPEG movie. To that observer, the behavior of every pixel would appear governed by deterministic,
periodic $\cos(\theta)$ functions. They might conclude that the ``laws'' of their universe are fundamentally wave-like, when in fact, they are simply observing the most efficient informational
basis for representing reality. Similarly, the wave-like behavior of quantum particles may simply be the result of nature utilizing a Fourier-based basis to minimize the computational cost of the vacuum.

While we perceive particles as ``reality,'' the wavefunction is the compression scheme that makes that reality computationally and informationally manageable. This is why everything we observe at the micro-scale ``waves.'' 

We are observing compressed structure.
