\chapter{From Nothing to Something}

\section{The Time-Reversed Black Hole}

If entropy collapse corresponds to geometric compression, then increasing entropy should correspond to geometric unfolding.
Cosmic expansion would be the time-reversed analogue of black hole formation. It is not an explosion of matter into a pre-existing space, but the progressive
differentiation of information that we perceive geometrically as increasing spatial separation. Collapse and expansion are simply two directions within the same informational configuration space.

\section{Entropy as a Driver of Expansion}

This is easy to simulate; instead of simulating black hole collapse and observing how the bitstring (execution trace) entropy approaches zero, one starts from
zero entropy execution trace and mutate it to introduce entropy. The corresponding 3D space should then unfold from singularity into some sort of virtual universe.


\subsection{Geometric Particle Filter}

In the black hole simulation we considered all dust particles as infinitely small points. However, in this simulation we pay attention 
not only to global spacetime but also emergent micro structures.

We define an execution trace
\[
S_t \in \{0,1\}^L, \quad t = 0, \dots, n,
\]
with fixed length $L$. The system is initialized at zero Shannon entropy,
\[
S_0 = (0,0,\dots,0),
\]
which we identify with the informational and geometric singularity.

As the system evolves through random bit-flip mutations, its expected Shannon entropy
\[
H(S_t) = - \sum_{b \in \{0,1\}} p_t(b)\log p_t(b)
\]
increases monotonically.

To visualize this process, we introduce a decoding map
\[
D : \{0,1\}^L \rightarrow \mathbb{R}^3,
\]
which assigns subsets of bits to spatial coordinates, producing a discrete spacetime fabric. On this induced geometry, we apply simple structural filters:

\begin{itemize}
    \item \textbf{Elementary particles:} Pairs of spatial points whose separation is below a threshold $\varepsilon$.
    \item \textbf{Atoms:} Triplets of points forming tight, approximately equilateral configurations.
    \item \textbf{Molecules:} Clusters of atoms whose geometric centers lie within a small separation threshold.
\end{itemize}

While arbitrary, the model preserves two key features characteristic of the real universe: (i) the hierarchical structuring of particles, and (ii) the fact that particle sizes do not stretch with the universe, only their mutual distances. This is what we observe in the real universe.

These filters are deliberately minimal and arbitrary. Their purpose is not realism, but robustness: to test whether structure emerges generically under entropy growth.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/entropy_to_matter.png}
    \caption{Structure count vs. entropy. The emergence follows a lognormal-like distribution.}
    \label{fig:entropy-matter}
\end{figure}

The following key properties are observed:

\begin{itemize}
    \item At zero entropy the spacetime geometry is a point, and no elementary particles emerge - this is the initial singularity.
    \item As entropy increases, the space unfolds exponentially.
    \item At higher entropy, elementary particles, atoms and molecules begin to appear, with their counts following lognormal-like trend.
\end{itemize}

\subsection{The Lognormal Signature}

Remarkably, across multiple mappings, decoding schemes, representations and threshold choices,
the abundance of emergent structures is not linear but follows a \textbf{lognormal distribution}.

Lognormal distributions often arise in physics, generically in systems governed by multiplicative stochastic processes, where growth proceeds through successive random amplifications
rather than additive steps. They are observed in phenomena as diverse as:

\begin{itemize}
    \item particle clustering,
    \item galaxy mass distributions,
    \item biological growth,
    \item economic and network hierarchies.
\end{itemize}


\subsection{Recursive Pattern Matching}

An alternative definition of emergent particles is given by recursive bit-pattern detection. Here, the execution trace $S_t$ itself is treated as a one-dimensional candidate space of particles, with no explicit geometric mapping required.

\textbf{Elementary particles:} Defined as short substrings $p \in \{0,1\}^k$ that occur within $S_t$.

\textbf{Composite particles:} Formed recursively by concatenating previously defined particles. A composite particle $P$ is valid if it appears in $S_t$ and if its sub-patterns are recognized particles.

This recursive pattern-matching approach captures the emergence of particles purely from informational redundancy, without reliance on an explicit geometric embedding. The hierarchy is constructed bottom-up: from frequent substrings (elementary particles), to composite concatenations (atoms), to repeated higher-order motifs (molecules).

As with the geometric detection method, two key features of physical reality are reproduced: (i) hierarchical structuring of matter, and (ii) invariance of particle size with respect to global expansion.

By counting the number of recursive particles at each entropy level, one again observes that zero entropy corresponds to no detectable structures, while higher entropy states give rise to exponentially growing numbers of particles whose abundances follow lognormal-like distribution.


\section{Entropy, Bit Flips, and the Shape of the Distribution}

When the abundance of emergent structures is plotted as a function of the number of random bit flips applied to the execution trace, the resulting distribution is \textbf{lognormal}.

This is actually expected: each bit flip multiplicatively increases the number of accessible configurations, and structure formation proceeds through stochastic multiplicative growth constrained by minimal coherence filters.

However, Shannon entropy itself is a logarithmic measure of configuration count. Entropy grows approximately as the logarithm of the number of accessible microstates.
Consequently, when the same structural abundance is plotted as a function of Shannon entropy rather than raw mutation count, the distribution becomes approximately \textbf{Gaussian}.


\section{Self-Location Within the Distribution}

If an observer is identified with one of the stable informational motifs selected by the structural filters—whether interpreted geometrically as a bound configuration or informationally as a persistent substring—then the observer is itself a sample drawn from this distribution.

Where, then, should such an observer expect to find itself? The answer is near the peak of the Gaussian curve, where the probability of emergent structures is the highest.

This is not an anthropic assumption imposed from the outside, but a direct consequence of self-location within the ensemble. Conditioned on being a stable structure at all, an observer is overwhelmingly likely to find itself near the maximum of the distribution, where the density of such structures is greatest.

From this perspective, the observed regularity, scale hierarchy, and apparent fine balance of the universe are not surprising. They are the typical properties of configurations sampled near the peak of an entropy-conditioned Gaussian.

The universe appears neither maximally ordered nor maximally random because neither extreme supports persistent observers. We find ourselves precisely where the statistics say we should:
near the maximum of structural abundance, at the crest of the informational wave.



\section{Probabilistic Theory of Everything}


This model is philosophically appealing because it removes the need for metaphysics. In this framework, the universe we observe is simply the most probable outcome—a result driven entirely by the laws of statistical mechanics, which we can fully understand and simulate.

The question then remains: can this informational model provide the unified explanatory framework that has eluded physics for a century?

\subsection{The Fine-Tuning Problem}

This model possesses several intriguing features. It predicts that observers will inevitably find themselves in a universe that appears optimally fine-tuned to support their existence. Furthermore, it offers an explanation for why, after approximately 13.8 billion years, the universe continues to expand near the critical rate while avoiding premature gravitational collapse.

\subsection{Low Entropy Initial State}

The early universe is widely recognized as a state of extremely low entropy, a condition that underlies the observed arrow of time and the thermodynamic evolution of cosmological systems \cite{penrose2010, carroll2010}. Classical analyses of cosmology and black hole physics emphasize that this initial low-entropy state is essential for the emergence of structure and temporal asymmetry \cite{hawking1988, hawking1996nature}.

While standard inflationary models typically invoke scalar fields or vacuum potentials to explain early exponential expansion, they often treat the low-entropy initial state as a fine-tuned boundary condition rather than a derived property. In contrast, this information-theoretic perspective treats spacetime geometry as a projection of an evolving execution trace. Here, the initial near-zero entropy state naturally generates a rapid, inflation-like expansion as the system relaxes toward higher entropy; exponential growth emerges as a generic combinatorial consequence of the system moving away from a state of maximal constraint.

\subsection{Inflation}

During its earliest moments, the universe underwent an inflationary period—a phase of rapid expansion that later transitioned into the currently observed Hubble flow. This inflationary phase appears to match the generic behavior of an information-theoretic system evolving from a zero-entropy execution trace with remarkable accuracy.

\subsection{Gravity}

Gravity may be interpreted as the geometric manifestation of the probability distribution of emergent microstructures under increasing entropy. Structures tend to move toward regions of higher statistical weight because those configurations are more numerous. Consequently, complex systems are statistically more likely to evolve toward such regions.

To put it intuitively: we 'fall' toward a specific region because there are more ways for us to exist 'down there' than 'up here.' Gravity, then, is not a pull, but a statistical inevitability—a macroscopic drift toward the most probable distribution of information.


\section{Open Probelms}

Desite these promising properties, the model suffers from several flaws.

\subsection{Thermodynamical Arrow and the Banana Paradox}

The model does not explain why the entropy increases with time rather than decreases.

Can intelligence even function in a system with decreasing entropy?

We consume low-entropy fuel—like a highly ordered carbohydrate structure (a banana)—and our bodies break that order down into high-entropy metabolic waste.
How would we harvest energy in decreasing entropy universe,
where things tend from noise towards higher order? To maintain the local arrow of time, we might essentially have to consume metabolic waste and, through some miracle of reverse-metabolism, ejaculate a perfectly formed, low-entropy banana.

While mathematically possible in a symmetric system, our current forward-running universe is certainly more aesthetically appealing. We are lucky enough to be born on the side of the curve where breakfast goes in the top and comes out as waste at the bottom, rather than the other way around.


\subsection{Bolzzman Brain}

Computational simulations can be utilized to stress-test this hypothesis, but they reveal a significant challenge: The Boltzmann Brain Problem. While structures do emerge within the model, they fail to transition smoothly along the geodesics of geometric spacetime. Instead of beautiful spiral galaxies or planets tracing elegant elliptical orbits, the simulation produces a chaotic 'explosion' of spacetime. No GR emerges. Wavefunction is missing. What we observe are stochastic, erratic structures that behave haphazardly, adhering only to a statistical probability gradient rather than the lawful patterns of classical physics.

This contradicts empirical observation; the universe overwhelmingly favors large, lawful, and persistent structures over isolated, transient fluctuations.

Within this purely probabilistic model is difficult to explain how features like inertia of mass could emerge. 


\subsection{The Great Coin Flip in the Dark}

And then, who flips the bits?  We reject any metaphysical or super natural flipper here. The goal is to understand the universe, not move those parts of the universe ``outside'' that we don't understand.
