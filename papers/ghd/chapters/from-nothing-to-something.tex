\chapter{From Nothing to Something}

\section{The Time-Reversed Black Hole}

If entropy collapse corresponds to geometric compression, then increasing entropy should correspond to geometric unfolding.
Cosmic expansion would be the time-reversed analogue of black hole formation. It is not an explosion of matter into a pre-existing space, but the progressive
differentiation of information that we perceive geometrically as increasing spatial separation. Collapse and expansion are simply two directions within the same informational configuration space.

\section{Entropy as a Driver of Expansion}

This is easy to simulate; instead of simulating black hole collapse and observing how the bitstring (execution trace) entropy approaches zero, one starts from
zero entropy execution trace and mutate it to introduce entropy. The corresponding 3D space should then unfold from singularity into some sort of virtual universe.


\subsection{Geometric Particle Filter}

In the black hole simulation we considered all dust particles as infinitely small points. However, in this simulation we pay attention 
not only to global spacetime but also emergent micro structures.

We define an execution trace
\[
S_t \in \{0,1\}^L, \quad t = 0, \dots, n,
\]
with fixed length $L$. The system is initialized at zero Shannon entropy,
\[
S_0 = (0,0,\dots,0),
\]
which we identify with the informational singularity.

As the system evolves through random bit-flip mutations, its expected Shannon entropy
\[
H(S_t) = - \sum_{b \in \{0,1\}} p_t(b)\log p_t(b)
\]
increases monotonically.

To visualize this process, we introduce a decoding map
\[
D : \{0,1\}^L \rightarrow \mathbb{R}^3,
\]
which assigns subsets of bits to spatial coordinates, producing a discrete spacetime fabric. On this induced geometry, we apply simple structural filters:

\begin{itemize}
    \item \textbf{Elementary particles:} Pairs of spatial points whose separation is below a threshold $\varepsilon$.
    \item \textbf{Atoms:} Triplets of points forming tight, approximately equilateral configurations.
    \item \textbf{Molecules:} Clusters of atoms whose geometric centers lie within a small separation threshold.
\end{itemize}

While arbitrary, the model preserves two key features characteristic of the real universe: (i) the hierarchical structuring of particles, and (ii) the fact that particle sizes do not stretch with the universe, only their mutual distances.

These filters are deliberately minimal and arbitrary. Their purpose is not realism, but robustness: to test whether structure emerges generically under entropy growth.

The following key properties are observed:

\begin{itemize}
    \item At zero entropy the spacetime geometry is a point, and no elementary particles emerge - this is the initial singularity.
    \item As entropy increases, the space unfolds exponentially.
    \item At higher entropy, elementary particles, atoms and molecules begin to appear, with their counts following lognormal-like trend.
\end{itemize}

\subsection{The Lognormal Signature}

The crucial result appears when one counts the number of detected structures as a function of bit flips. Across multiple mappings, decoding schemes, representations and threshold choices,
the abundance of emergent structures is not linear but follows a \textbf{lognormal distribution}.

Lognormal distributions often arise in physics, generically in systems governed by multiplicative stochastic processes, where growth proceeds through successive random amplifications
rather than additive steps. They are observed in phenomena as diverse as:

\begin{itemize}
    \item particle clustering,
    \item galaxy mass distributions,
    \item biological growth,
    \item economic and network hierarchies.
\end{itemize}

In the present framework, the lognormal distribution emerges naturally from a randomly mutated bitstring. Each additional bit of entropy multiplies the number of available configurations,
while structural filters select stable combinations. The result is a characteristic peak: too little entropy yields no structure; too much entropy destroys stability; maximal structure appears in between.


\subsection{Recursive Pattern Matching}

An alternative definition of emergent particles is given by recursive bit-pattern detection. Here, the execution trace $S_t$ itself is treated as a one-dimensional candidate space of particles, with no explicit geometric mapping required.

\textbf{Elementary particles:} Defined as short substrings $p \in \{0,1\}^k$ that occur within $S_t$.

\textbf{Composite particles:} Formed recursively by concatenating previously defined particles. A composite particle $P$ is valid if it appears in $S_t$ and if its sub-patterns are recognized particles.

This recursive pattern-matching approach captures the emergence of particles purely from informational redundancy, without reliance on an explicit geometric embedding. The hierarchy is constructed bottom-up: from frequent substrings (elementary particles), to composite concatenations (atoms), to repeated higher-order motifs (molecules).

As with the geometric detection method, two key features of physical reality are reproduced: (i) hierarchical structuring of matter, and (ii) invariance of particle size with respect to global expansion.

By counting the number of recursive particles at each entropy level, one again observes that zero entropy corresponds to no detectable structures, while higher entropy states give rise to exponentially growing numbers of particles whose abundances follow lognormal-like distribution.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/entropy_to_matter.png}
    \caption{Structure count vs. entropy. The emergence follows a lognormal-like distribution.}
    \label{fig:entropy-matter}
\end{figure}


\section{Entropy, Bit Flips, and the Shape of the Distribution}

When the abundance of emergent structures is plotted as a function of the number of random bit flips applied to the execution trace, the resulting distribution is \textbf{lognormal}.
This is expected: each bit flip multiplicatively increases the number of accessible configurations, and structure formation proceeds through stochastic multiplicative growth constrained by minimal coherence filters.

However, Shannon entropy itself is a logarithmic measure of configuration count. Entropy grows approximately as the logarithm of the number of accessible microstates.
Consequently, when the same structural abundance is plotted as a function of Shannon entropy rather than raw mutation count, the distribution becomes approximately \textbf{Gaussian}.

The lognormal form reflects the multiplicative nature of random informational growth in configuration space, while the Gaussian form reflects the same process expressed in the natural logarithmic coordinate defined by entropy.
No additional assumptions are required: the Gaussian shape emerges directly from the definition of entropy as a logarithmic measure.

In other words, the apparent lognormal distribution in mutation space and the Gaussian distribution in entropy space are two representations of the same underlying statistical structure.



\section{Self-Location Within the Distribution}

If an observer is identified with one of the stable informational motifs selected by the structural filters—whether interpreted geometrically as a bound configuration or informationally as a persistent substring—then the observer is itself a sample drawn from this distribution.

Where, then, should such an observer expect to find itself? The answer is near the peak of the Gaussian curve, where the probability of emergent structures is the highest.

This is not an anthropic assumption imposed from the outside, but a direct consequence of self-location within the ensemble. Conditioned on being a stable structure at all, an observer is overwhelmingly likely to find itself near the maximum of the distribution, where the density of such structures is greatest.

From this perspective, the observed regularity, scale hierarchy, and apparent fine balance of the universe are not surprising. They are the typical properties of configurations sampled near the peak of an entropy-conditioned Gaussian.

The universe appears neither maximally ordered nor maximally random because neither extreme supports persistent observers. We find ourselves precisely where the statistics say we should:
near the maximum of structural abundance, at the crest of the informational wave.


\section{Open Problems}

Could this statistical informational model provide a unified explanatory framework?

Computational simulations can be utilized to stress-test the hypothesis. These simulations reveal a significant challenge: The Boltzmann Brain Problem. While structures do emerge within the model, they do not transition smoothly along geodesics through geometric spacetime. Instead, they appear stochastically, adhering only to a statistical probability gradient. This contradicts empirical observation; the universe overwhelmingly favors large, lawful, and persistent structures over isolated, transient fluctuations.
\section{Conclusions}
\subsection{The Fine-Tuning Problem}

Despite the Boltzmann Brain problem, this model possesses several intriguing features. It predicts that observers will inevitably find themselves in a universe that appears optimally "fine-tuned" to support their existence. Furthermore, it offers an explanation for why, after approximately 13.8 billion years, the universe continues to expand near the critical rate while avoiding premature gravitational collapse.
\subsection{Low Entropy Initial State}

The early universe is widely recognized as a state of extremely low entropy, a condition that underlies the observed arrow of time and the thermodynamic evolution of cosmological systems \cite{penrose2010, carroll2010}. Classical analyses of cosmology and black hole physics emphasize that this initial low-entropy state is essential for the emergence of structure and temporal asymmetry \cite{hawking1988, hawking1996nature}.

While standard inflationary models typically invoke scalar fields or vacuum potentials to explain early exponential expansion, they often treat the low-entropy initial state as a fine-tuned boundary condition rather than a derived property. In contrast, this information-theoretic perspective treats spacetime geometry as a projection of an evolving execution trace. Here, the initial near-zero entropy state naturally generates a rapid, inflation-like expansion as the system relaxes toward higher entropy; exponential growth emerges as a generic combinatorial consequence of the system moving away from a state of maximal constraint.
\subsection{Inflation}

During its earliest moments, the universe underwent an inflationary period—a phase of rapid expansion that later transitioned into the currently observed Hubble flow. This inflationary phase appears to match the generic behavior of an information-theoretic system evolving from a zero-entropy execution trace with remarkable accuracy.
\subsection{Final Verdict}

Gravity may be interpreted as the geometric manifestation of the probability distribution of emergent microstructures under increasing entropy. Structures tend to move toward regions of higher statistical weight because those configurations are more numerous. Consequently, complex systems are statistically more likely to evolve toward such regions.

To put it intuitively: we 'fall' toward a specific region because there are more ways for us to exist 'down there' than 'up here.' Gravity, then, is not a pull, but a statistical inevitability—a macroscopic drift toward the
most probable distribution of information.

However, the model currently lacks a mechanism to suppress the Boltzmann Brain problem and ensure the continuity of macroscopic structures.
