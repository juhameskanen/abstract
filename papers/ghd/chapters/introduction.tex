\chapter{Introduction}

\section{Advances in Science}

During the past centuries, physics has achieved remarkable success in unifying a large number of partial theories into two powerful frameworks: Quantum Mechanics and General Relativity. The equations in both theories match all observations with remarkable precision, limited only by current technological capabilities. Those capabilities themselves have reached a level that would have seemed almost inconceivable only a few decades ago.

Gravitational-wave observatories such as LIGO can detect distortions of spacetime smaller than a proton’s diameter, measuring relative changes in length caused by distant black-hole mergers billions of light-years away. Atomic clocks, exploiting the quantum structure of atoms, now keep time so precisely that they would lose or gain less than a second over the age of the universe, and are sensitive enough to register differences in gravitational potential corresponding to changes in height of mere centimeters.

Elsewhere, quantum electrodynamics predicts the magnetic moment of the electron to a precision verified to many decimal places, making it one of the most accurately tested theories in all of science. Interferometers routinely resolve wavelengths far smaller than the structures they probe, while particle accelerators recreate conditions not seen since the earliest moments after the Big Bang. Even the global positioning systems that guide everyday navigation rely on relativistic corrections so precise that neglecting them would lead to kilometer-scale errors within a single day.

These achievements are not isolated triumphs but manifestations of a deeper pattern: our theoretical descriptions of nature have become so accurate that reality itself now serves as the experimental apparatus for testing them. Physical law is no longer merely inferred from observation; it is continuously confirmed, corrected, and operationalized by technologies that depend on its validity to function at all.

It is against this backdrop of unprecedented precision and control that new questions arise. When our ability to manipulate matter, energy, time, and information approaches the fundamental limits set by physics, distinctions that once seemed clear—between simulation and reality, computation and physical process, description and instantiation—begin to blur. The success of modern physics, paradoxically, forces us to confront not only how well we understand the universe, but how far that understanding can be carried into domains once thought to lie beyond its reach.

What is perhaps less often emphasized is how deeply these theories have escaped the confines of paper and chalk, becoming embedded in the technological fabric of modern civilization. The most striking example is computation. From the quantum-mechanical behavior of transistors to the relativistic corrections required for satellite navigation, our deepest physical theories now operate continuously and invisibly inside machines that process information at planetary scale. Computation is no longer merely a tool for studying nature; it has become a physical process in its own right, governed by energy constraints, thermodynamics, noise, and quantum limits.

In recent decades, this trajectory has culminated in the rise of artificial intelligence systems of unprecedented complexity. These systems are not programmed in the traditional sense but are shaped through optimization processes that resemble physical evolution more than logical deduction. Trained on vast datasets and executed on hardware operating near fundamental physical limits, they exhibit behaviors—learning, abstraction, creativity—that were once considered exclusively biological. Remarkably, their success does not rely on new physical laws, but on exploiting known ones at scale, transforming raw energy into structured information with extraordinary efficiency.

In addition to significantly enhancing our understanding of how the universe works, science has also bestowed upon humanity a vast
array of practical applications. These range from the development of microprocessors to the creation of GPS systems, 
not  to mention nuclear bombs.

Science has much to celebrate. 


\section{The Elusive Theory of Everything}

Following the success of unifying partial theories into two grand pillars—General Relativity (GR) and Quantum Mechanics (QM)—it appeared certain that the unification process would eventually reach the ultimate goal of physics: a Theory of Everything. This would be a single equation capable of describing the entire universe. To date, however, this has not happened. Despite overwhelming evidence supporting both theories, they remain fundamentally incompatible; they cannot both be correct.

\section{Which One Is Right?}

Historically, most attempts at unification assume that the quantum description is more fundamental, so it is General Relativity that should be modified, because everything else has already been quantized. Matter fields—electrons, photons, quarks—all obey quantum field theory. Spacetime might simply be another field awaiting quantization, and several facts appear to support this view.

First, GR breaks down at small scales. Near singularities or at the Planck length, curvature becomes infinite. This signals a failure of the continuum picture, not of quantum mechanics. The intuition is therefore to quantize gravity to remove these divergences, just as quantizing electromagnetism resolved the ultraviolet catastrophe.

However, despite decades of research, no single framework has yet succeeded in combining the principles of quantum mechanics with the geometric description of spacetime provided by General Relativity.

\subsection{Semiclassical Gravity}

One of the earliest attempts to bridge the quantum--classical divide is \emph{semiclassical gravity}. In this approach, matter is treated as fully quantum, while spacetime remains classical. To make the Einstein field equations workable, the operator-valued stress--energy tensor of quantum matter is replaced by its expectation value—the renormalized average of the energy and momentum calculated over the quantum state of the matter fields. This resulting set of ordinary numbers can then be inserted into the equations governing curvature.

Semiclassical gravity is remarkably successful. It accurately describes a wide range of phenomena, from laboratory experiments to astrophysical observations and cosmology. It even predicts striking effects such as Hawking radiation in black holes. Yet its very success also exposes its conceptual limitation: the approach is \emph{ad hoc}. Spacetime geometry is treated as classical while matter is quantum. The formal mismatch remains, and semiclassical gravity gives no guidance on what a fully quantum spacetime might look like. In other words, it works well for everything we can observe—but it does not answer the deeper question: how should spacetime itself be quantized?

\subsection{Perturbative Quantum Gravity}

A natural next step is \emph{perturbative quantum gravity}, where spacetime is expanded around a simple background—typically flat or slightly curved—and the perturbations are treated as quantum fields. This approach is conceptually straightforward and extends the familiar machinery of quantum field theory to gravity.

However, it quickly runs into a fundamental problem: gravity is \emph{nonrenormalizable}. Unlike the Standard Model, where infinities in quantum corrections can be controlled through renormalization, attempts to remove infinities in perturbative quantum gravity fail. The equations produce uncontrolled divergences, and no systematic procedure yields finite, predictive results. The techniques that work spectacularly well for matter fields simply break down for spacetime itself.

\subsection{Nonperturbative and Geometric Approaches}

In response to the failure of perturbative quantization, researchers have developed nonperturbative frameworks that do not assume a fixed background geometry. A leading example is Loop Quantum Gravity (LQG), which models spacetime as a discrete combinatorial structure of spin networks. LQG is mathematically rigorous and fully background-independent, offering a conceptually clean quantization of geometry.

However, major obstacles remain. Deriving a smooth classical spacetime limit is nontrivial, and embedding standard particle physics into the LQG framework remains unresolved. Likewise, Causal Dynamical Triangulations (CDT) reconstruct spacetime via a sum over discrete simplicial geometries and can reproduce some large-scale features of our universe, but they are computationally demanding and treat matter fields awkwardly.

Taken together, these approaches illustrate a persistent tension: nonperturbative quantization can yield consistent quantum geometries in principle, but recovering observed macroscopic physics from them remains an open challenge.

\subsection{String Theory: A Unified but Unverified Framework}

Another major avenue is string theory, in which point particles are replaced by one-dimensional strings. Gravity emerges as one of the vibrational modes of the string, and the framework unifies all fundamental forces in principle. String theory has revealed deep mathematical structures, including dualities, extra dimensions, black-hole entropy counting, and insights into holography.

However, significant challenges remain. Most phenomenologically viable constructions rely on supersymmetry, which has not been observed experimentally. The theory admits an enormous landscape of possible vacuum states—often estimated at around $10^{500}$—raising concerns about predictivity and falsifiability. Extra spatial dimensions are required, typically assumed to be compactified at extremely small scales, yet they remain empirically undetected. Direct experimental tests of string-scale physics are effectively out of reach.

A theory sufficiently flexible to accommodate all observations risks explaining none. Whether string theory ultimately avoids this fate remains an open question.

\subsection{Emergent and Holographic Approaches}

A more radical class of ideas treats gravity and spacetime as emergent rather than fundamental. This perspective arose from puzzles at the intersection of gravity, thermodynamics, and quantum theory. Black holes behave as thermodynamic objects, possessing entropy proportional to horizon area and emitting thermal radiation. These results suggest a deep link between geometry, information, and statistical mechanics.

The observation that gravitational entropy scales with area rather than volume led to the holographic principle: the idea that the degrees of freedom of a region of spacetime may be encoded on its boundary. Holographic dualities further support this view, showing that spacetime geometry and gravitational dynamics can emerge from nongravitational quantum theories.

\subsubsection{From Geometry to Bitstring}

The holographic principle argues that a three-dimensional universe can be described by a two-dimensional theory ($N \rightarrow N-1$). If $3D$ can map to $2D$, why stop there?

In software architecture, any $N$-dimensional space is ultimately stored as a one-dimensional bitstring ($N \rightarrow 1$). A sequence of bits has no intrinsic geometry; width and height are formatting conventions imposed by interpretation. Following this logic to its conclusion, the universe may be fundamentally dimensionless. Space, curvature, and connectivity are rendering modes of a one-dimensional sequence of axiomatic instructions.

In this view, entanglement reflects proximity in information space, not spatial distance in a geometric projection.

\subsection{Common Problem}

Across all approaches, a common issue persists: the mismatch between quantum matter and classical spacetime. Attempts to resolve this either sidestep the problem, fail technically, or sacrifice predictive power. As a result, some have questioned whether a Theory of Everything is possible.

Stephen Hawking argued that our position as observers embedded within the universe may impose fundamental limits. Gödel’s incompleteness theorem suggests that any sufficiently rich self-referential system must be incomplete or inconsistent. A team led by Mir Faizal argued that a complete simulation of reality is impossible on similar grounds~\cite{faizal2025}.

\section{The Deeper Problem: Unexplained Assumptions}

All candidate theories rely on unexplained axioms. This incompleteness can be expressed as
\[
\text{ToE}_{\text{incomplete}} = \text{ToE}_{\text{ideal}} \setminus \mathcal{A},
\]
where $\mathcal{A}$ denotes the set of unexplained axioms.

General Relativity assumes spacetime; quantum field theory assumes fields and constants; string theory assumes extra dimensions and vast landscapes. These theories explain behavior, not existence.

A genuine Theory of Everything should explain why the framework itself exists, why laws take the form they do, and why stable observers arise at all.

\section{Human Conduct: Pain}

Finally, a Theory of Everything must address subjective experience. Physics contains no equation for pain, no particle of consciousness. 

Will science and physics ever be capable of answering these kind of profound big questions, or must we seek answers elsewhere?

