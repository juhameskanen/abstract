\chapter{Spectral Occam’s Razor}

\section{The Problem With ``Something from Nothing''}

One can arrange $n$ bits of information in $2^n$ ways and sort those configurations in $(2^n)!$ ways.
This can be viewed as a set of videos, each with a capacity of $n$ bits—a single ``movie.''
One of these movies might be a $2 \times 2$ pixel video with a vast number of frames; another might be an extremely high-density, single-frame still image. 

Undoubtedly, one of these videos depicts a structure describing the execution trace of an intelligent observer wandering through time and an expanding universe.
That observer might eventually seek an explanation for its own existence---specifically, why the pixels in that movie appear to be so delicately arranged as to make the observer's existence possible.

However, this \textbf{Anthropic Principle}---the idea that things appear as they do simply because we wouldn't be here to observe them otherwise---lacks predictive power. Furthermore, the number of such coherent configurations is vanishingly small compared to the total number of possible configurations. The probability of finding ourselves on such a path by pure chance is practically zero under a uniform measure over all configurations.

Moreover, it remains difficult to explain why these randomly emergent structures, which we label as matter and energy, would exhibit fundamental behaviors like mutual attraction. 

\section{Assumptions}

Let us assume the universe is informational by nature. Singularities inside black holes and in our past (the Big Bang) are no longer pathological mysteries but fully understood points of minimal information. More notably, after decades of serious effort, the unification of General Relativity (GR) and Quantum Mechanics (QM) has not achieved a satisfactory solution. Let us accept the possibility that these concepts are merely mutually orthogonal compressed descriptions of reality.

This leads us to the following fundamental building blocks:

\begin{itemize}
  \item \textbf{Particle View:} A finite, discrete, inflated description of localized phenomena.
  \item \textbf{Quantum Mechanics:} A spectral, infinitely smooth, and continuous compressed description.
  \item \textbf{General Relativity:} A geometric compressed description of spacetime topology.
\end{itemize}

Why compression? Why do we observe a universe that appears obsessed with minimizing energy and resources? From the path of a photon to the orbits of planets, physical systems tend toward their minimal energy equilibria. If the universe is fundamentally informational, ``energy'' translates roughly into \textit{information density}, and ``equilibrium'' translates into \textit{maximal compression}.

\section{Kolmogorov Complexity and Solomonoff's Prior}

There is a well-established principle called \textbf{Kolmogorov Complexity} $K(x)$. It defines the information content of a string $x$ as the length of the shortest program $p$ that can reproduce $x$ on a Universal Turing Machine (UTM):

\begin{equation}
K(x) = \min \{ |p| : U(p) = x \}.
\end{equation}

While $K(x)$ measures the complexity of an individual object, \textbf{Solomonoff's Universal Prior} $P(x)$ provides a framework for induction. It assigns a probability to a sequence based on the likelihood that a random program will produce it. A critical feature of this measure is that the \textbf{Minimal Description Length (MDL) dominates the measure}. Because the probability of a program of length $|p|$ is $2^{-|p|}$, the shortest programs contribute exponentially more to the total probability:

\begin{equation}
P(x) = \sum_{p: U(p)=x} 2^{-|p|} \approx 2^{-K(x)},
\end{equation}

where the approximation holds up to a multiplicative constant.

\subsection{Landauer's Principle and the Physics of Information}

The bridge between $K(x)$ and physical reality is \textbf{Landauer's Principle}. It asserts that the erasure of information is a dissipatively irreversible process. To erase one bit of information, a system must dissipate a minimum amount of heat:

\begin{equation}
\Delta Q \ge k_B T \ln 2.
\end{equation}

This suggests that the thermodynamic entropy of a system is bounded by its informational complexity. However, $K(x)$ presents two significant challenges for physical modeling:

\begin{itemize}
    \item \textbf{Uncomputability:} Due to the Halting Problem, a Turing Machine cannot determine the absolute shortest program for a string.
    \item \textbf{Non-continuity:} $K(x)$ is not a smooth function; infinitesimal changes in input can yield massive jumps in program size.
\end{itemize}

We don't observe discrete non-continuous universe. Physical theories such as GR and QM are built on continuity,
whereas Kolmogorov Complexity is fundamentally discrete. This discrepancy motivates a continuous replacement.

\section{The Continuous Replacement to Kolmogorov}

Why does the universe, at its microscopic level, appear to ``wave'' according to the abstract deterministic Wavefunction? The answer is that we are observing compressed structures.
If we treat the laws of physics not as a discrete Turing Machine, but as a \textbf{Spectral Compression Algorithm}, the wavefunction becomes the mechanism of efficiency.
Unlike the discrete nature of $K(x)$, Fourier-based compression is both countable and allows continuous amplitudes and phases.

Instead of counting bits, we view the universe as a \textbf{Harmonic Processor} counting basis states. A wavefunction $\Psi$ can be decomposed into frequencies and phases:

\begin{equation}
\Psi = \sum_{n=1}^{N} A_n \exp(i(k_n x - \omega_n t + \phi_n)),
\end{equation}

where $k_n$, $\omega_n$, and $\phi_n$ are determined by boundary and quantization conditions. The ``informational size'' of this state is defined by the number of active components $N$. We can thus establish a \textbf{Spectral Prior}:

\begin{equation}
P(\Psi) \propto 2^{-N}.
\end{equation}

A pure cosine wave ($N=1$) is the simplest possible configuration, making it the most ``informationally affordable'' and thus the most probable building block of reality. Under this view, physical laws are the statistical consequence of a universe that favors \textbf{spectral sparsity}.

To help visualize this idea, consider familiar signal processing scenarios, such as textbf{Audio Compression (MP3):} A pure tone is represented by very
few frequencies and compresses extremely efficiently. A complex symphony requires many frequencies, increasing its informational size.
The universe behaves like an MP3 encoder: states that can be represented with fewer spectral components are far more probable.

textbf{Image Compression (JPEG):} is another example. Smooth gradients or repeating patterns require fewer Fourier or DCT coefficients to encode, while noisy
textures require many. Similarly, physical reality favors ``smooth'' configurations that minimize spectral complexity.
    
These examples illustrate why, at a microscopic level, the universe appears to ``wave'' smoothly rather than behaving like random noise.
The wavefunction encodes the minimal spectral resources needed to represent observer-relevant histories, making spectral sparsity a natural outcome.


\subsection{The Convergence of Solomonoff and Gibbs}

This reveals a deep symmetry between Information Theory and Statistical Mechanics. Solomonoff's Prior and the \textbf{Gibbs Measure} describe the same phenomenon of ``minimal cost.''

In Solomonoff's framework: $P(x) \sim 2^{-K(x)}$. In the Gibbs framework: $P(x) \sim e^{-\beta E}$.

If the energy of a wavefunction is proportional to its spectral size, these equations converge. A multi-component wavefunction is both energetically expensive and informationally complex. This implies that the \textbf{Principle of Least Action} is the physical manifestation of the \textbf{Minimal Description Length}. The universe settles into smooth functions because they represent the ``thermal equilibrium'' of information. They dominate the measure, and therefore, are the most probable.



