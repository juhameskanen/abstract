\chapter{Typical Software}

It is easy to imagine a powerful computer with a huge database and advanced logic. 
Such a system could be highly efficient in its operations, capable of making accurate and intelligent decisions in nearly any imaginable situation. 
However, it is difficult to see how such a mechanically operating machine could truly feel pain.

Imagine a typical software program consisting of thousands of lines of code. How many additional source lines would need to be added to 
transform the software into a conscious entity? Would it be the $10^{14}$th line that suddenly imbues the system with the ability to feel pain? 
Could it be the introduction of a deeply nested loop that finally grants consciousness? Or is it the number of \texttt{if-else} clauses that holds the secret?

Regardless of the number of loops and source lines added, it appears that nothing significant would occur. 
The software program would remain just that---a software program, albeit larger in size.

If software were truly capable of sensing pain, what would be the worst thing that could happen to it? 
Is it division by zero, or a reference to an uninitialized variable?

\begin{verbatim}
int uninitialized;
int initialized = 3;

int good = 2 * PI * initialized;   // feel good :)
int bad  = 2 * PI * uninitialized; // feel pain :(
int maximal_pain = 1/0;  // division by zero, maximal pain!
\end{verbatim}

If consciousness is not solely a software issue, could it be related to hardware instead? For example, the graphics board controls what the 
computer renders on its screen. By writing appropriate values to memory addresses constituting the so-called video memory, one can turn pixels 
on and off to create images. What would be the memory addresses one has to poke in order to create pain?

\begin{verbatim}
// try to poke pain
*((bool *)0x000000) = true; // argh
\end{verbatim}



\section{The Hard Problem of Consciousness}

A computer is a mechanical device whose operation can ultimately be reduced to the manipulation of its bits and components. 
Its elementary building blocks are typically electronic elements that are, in functional terms, equivalent to mechanical relays. 
The notion that a collection of relays connected in a network of copper wires could genuinely experience consciousness or feel pain is difficult to take seriously.

Should I type gently on my keyboard, fearing that striking the keys too hard might trigger a migraine in my laptop? 
Do partially broken memory chips introduce suffering, much like a broken tooth causes pain to its owner? 
Could hardware defects transform my happy computer into a suffering one, making it wish it were dead—or at least turned off?

How many additional relays would I need to add to my home automation system to produce pain? 
Or perhaps it is not the relays but the wires—should I replace copper with aluminium to generate suffering? 
Would three-phase relays instead of single-phase ones finally cross the threshold into consciousness?

As absurd as these examples may sound, they illustrate the problem clearly. 
There is not even the faintest hint of an explanation for how pain—or any other subjective human experience—could be implemented in software running on conventional hardware.

There is, however, an even deeper difficulty. Neuroscience has made remarkable progress in studying the operation of the human brain. 
The development of brain imaging techniques, such as magnetic resonance imaging (MRI), enables researchers to investigate the neurobiological correlates of behavior. 
Yet what is striking is that conscious experience does not appear reducible to the mere operation of the brain’s elementary physical components. 
It seems that subjective experience cannot be explained solely in terms of neural processes. 
This tension is known as the ``hard problem of consciousness''~\cite{chalmers1995facing}.

\section{Organic Tissue Issue}

Could consciousness lurk in the fact that humans are composed of organic biological tissue, \aside{such as celluloid}?, which is considered ``alive'' as opposed to non-organic matter like silicon? Hardly; both fat and silicon are ultimately made up of the very same type of subcomponents. 

Is all matter conscious to some degree, as panpsychism suggests? Could plants, trees, or even rocks have some level of 
consciousness~\cite{Goff2019,Strawson2006,Chalmers2015,Whitehead1929}?

The best imaginable way to study whether an object is conscious is by torturing it with an appropriate torturing device. 
So let us torture rocks with the best possible rock-torturing device one can imagine---a sledgehammer. 
Rocks do not seem to care! This observation cannot, of course, prove rocks unconscious. Rocks could well be conscious, 
they just do not have the sense to feel pain. Or perhaps they do sense pain intensely, but they just cannot show it. 
They might be in everlasting pain, but have no mouth to scream, no legs to kick. What a terrible destiny!


\section{Proposed Sources of Consciousness}

No consensus exists regarding the source of consciousness. Instead, proposals span nearly every possible scale of physical description.

At the macroscopic level, most neuroscientific theories locate consciousness in large-scale brain dynamics: coordinated neural firing, thalamocortical loops, or global workspace architectures.
In this view, consciousness is an emergent property of complex biological organization.

At smaller scales, some theories identify consciousness with specific cellular or subcellular mechanisms. The most famous example is the Orchestrated Objective Reduction (Orch-OR) model proposed by \citeauthor{penrose1989emperor} and \citeauthor{hameroff1996conscious}~\cite{penrose1989emperor,hameroff1996conscious}, which attributes conscious processes to quantum coherence in neuronal microtubules.

At the most reductionist end, certain approaches appeal directly to fundamental physics. Consciousness has been linked to quantum states, wave-function collapse, spacetime geometry, or even black hole singularities. In some cases, this leads to panpsychism—the view that consciousness is a basic feature of matter itself.

Finally, computational and functionalist theories argue that consciousness depends only on the right informational structure. According to this view, any system—biological or artificial—that implements the appropriate computation could, in principle, be conscious. Contemporary discussions of integrated information and artificial intelligence fall into this category.

The striking fact is not which of these theories is correct, but that every conceivable level of description—cosmic, quantum, cellular, neural, computational—has been proposed as the decisive one.
There is no agreed-upon scale, mechanism, or substrate. The dispersion of candidates is itself evidence of explanatory instability.
Consciousness does not merely lack a solution; it lacks a well-defined location in our ontology.

